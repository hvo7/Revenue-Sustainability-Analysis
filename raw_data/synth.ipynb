{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced5ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca94039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch-directml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013224f",
   "metadata": {},
   "source": [
    "### This file is used to synthesize data that mimics the existing patterns of a dataset. For project purposes sake, we can synthesize data to more realistically represent real-world data. \n",
    "\n",
    "This currently only applies to the feature_usage.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "base_dir = Path(\"C:\\\\Users\\\\henry\\\\OneDrive\\\\Personal Career\\\\Personal Projects\\\\GitHub\\\\Revenue-Sustainability-Analysis\")\n",
    "data_dir = Path(base_dir / \"raw_data/Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c11491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bce3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM '{data_dir}/ravenstack_feature_usage.csv'\n",
    "                 \"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b05c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Using device: privateuseone:0\n",
      "Tensor on AMD GPU: tensor([1., 1., 1.], device='privateuseone:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "# Check if DirectML is available\n",
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "    print(f\"Success! Using device: {device}\")\n",
    "    \n",
    "    # Create a tensor on your AMD GPU\n",
    "    x = torch.ones(3).to(device)\n",
    "    print(\"Tensor on AMD GPU:\", x)\n",
    "else:\n",
    "    print(\"DirectML not found. Check your installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c2fc06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sdv\\metadata\\single_table.py:835: UserWarning: There is an existing primary key 'usage_pk'. This key will be removed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sdv\\single_table\\base.py:134: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Sampling conditions: 100%|██████████| 198/198 [00:00<00:00, 2186.17it/s]\n"
     ]
    }
   ],
   "source": [
    "RELEASE_TS = pd.Timestamp(\"2025-10-01\")\n",
    "INTRODUCED_FEATURE = \"feature_new_ai\"\n",
    "TOTAL_ROWS = 50000\n",
    "POST_RELEASE_LIFT = 3.0\n",
    "TABLE = \"usage_events\"\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# types\n",
    "df[\"usage_date\"] = pd.to_datetime(df[\"usage_date\"], errors=\"coerce\")\n",
    "df[\"feature_name\"] = df[\"feature_name\"].replace(\"None\", pd.NA)\n",
    "df = df.dropna(subset=[\"feature_name\"])\n",
    "\n",
    "df[\"subscription_id\"] = df[\"subscription_id\"].astype(str)\n",
    "df[\"feature_name\"] = df[\"feature_name\"].astype(str)\n",
    "df[\"is_beta_feature\"] = df[\"is_beta_feature\"].astype(bool)\n",
    "\n",
    "for c in [\"usage_count\", \"usage_duration_secs\", \"error_count\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# release flag\n",
    "df[\"post_release\"] = (df[\"usage_date\"] >= RELEASE_TS)\n",
    "\n",
    "# ensure introduced feature exists\n",
    "if INTRODUCED_FEATURE not in set(df[\"feature_name\"].unique()):\n",
    "    seed_n = max(10, int(0.002 * len(df)))\n",
    "    seed = df.sample(seed_n, random_state=42).copy()\n",
    "    seed[\"feature_name\"] = INTRODUCED_FEATURE\n",
    "    seed[\"post_release\"] = True\n",
    "    seed[\"is_beta_feature\"] = False\n",
    "    df = pd.concat([df, seed], ignore_index=True)\n",
    "\n",
    "# CRITICAL: create guaranteed-unique PK AFTER all concatenations\n",
    "df[\"usage_pk\"] = [uuid.uuid4().hex for _ in range(len(df))]\n",
    "\n",
    "# metadata\n",
    "metadata = Metadata.detect_from_dataframe(data=df, table_name=TABLE)\n",
    "metadata.update_column(table_name=TABLE, column_name=\"usage_pk\", sdtype=\"id\")\n",
    "metadata.set_primary_key(table_name=TABLE, column_name=\"usage_pk\")\n",
    "metadata.update_column(table_name=TABLE, column_name=\"usage_date\", sdtype=\"datetime\")\n",
    "\n",
    "# fit\n",
    "synth = GaussianCopulaSynthesizer(metadata, enforce_min_max_values=True, enforce_rounding=True)\n",
    "synth.fit(df)\n",
    "\n",
    "# sample baseline\n",
    "syn_base = synth.sample(num_rows=TOTAL_ROWS)\n",
    "\n",
    "# oversample introduced feature post-release\n",
    "post_df = df[df[\"post_release\"] == True]\n",
    "post_share = len(post_df) / len(df) if len(df) else 0.5\n",
    "post_n = int(TOTAL_ROWS * post_share)\n",
    "\n",
    "base_feat_share_post = (post_df[\"feature_name\"] == INTRODUCED_FEATURE).mean() if len(post_df) else 0.0\n",
    "base_feat_n_post = int(post_n * base_feat_share_post)\n",
    "target_feat_n_post = int(base_feat_n_post * POST_RELEASE_LIFT)\n",
    "add_n = max(0, target_feat_n_post - base_feat_n_post)\n",
    "\n",
    "if add_n > 0:\n",
    "    cond = Condition({\"post_release\": True, \"feature_name\": INTRODUCED_FEATURE}, num_rows=add_n)\n",
    "    syn_lift = synth.sample_from_conditions([cond])\n",
    "    synthetic_df = pd.concat([syn_base, syn_lift], ignore_index=True)\n",
    "else:\n",
    "    synthetic_df = syn_base.copy()\n",
    "\n",
    "# final unique id for synthetic dataset\n",
    "synthetic_df[\"usage_id\"] = [f\"syn_usage_{i}\" for i in range(len(synthetic_df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d2b6e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>subscriptions_used</th>\n",
       "      <th>median_usage_count_per_sub</th>\n",
       "      <th>median_usage_duration_secs_per_sub</th>\n",
       "      <th>pct_subscriptions_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature_12</td>\n",
       "      <td>624</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3050</td>\n",
       "      <td>12.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>feature_26</td>\n",
       "      <td>616</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2905</td>\n",
       "      <td>12.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>feature_32</td>\n",
       "      <td>614</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3055</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feature_17</td>\n",
       "      <td>613</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2860</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>feature_34</td>\n",
       "      <td>613</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2933</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>feature_38</td>\n",
       "      <td>610</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2802</td>\n",
       "      <td>12.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>feature_2</td>\n",
       "      <td>607</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2870</td>\n",
       "      <td>12.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_11</td>\n",
       "      <td>606</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3058</td>\n",
       "      <td>12.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feature_15</td>\n",
       "      <td>606</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2874</td>\n",
       "      <td>12.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>feature_6</td>\n",
       "      <td>604</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2852</td>\n",
       "      <td>12.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  subscriptions_used  median_usage_count_per_sub  \\\n",
       "3    feature_12                 624                        10.0   \n",
       "18   feature_26                 616                        10.0   \n",
       "25   feature_32                 614                        10.0   \n",
       "8    feature_17                 613                        10.0   \n",
       "27   feature_34                 613                        10.0   \n",
       "31   feature_38                 610                        10.0   \n",
       "11    feature_2                 607                        10.0   \n",
       "2    feature_11                 606                        10.0   \n",
       "6    feature_15                 606                        10.0   \n",
       "36    feature_6                 604                        10.0   \n",
       "\n",
       "    median_usage_duration_secs_per_sub  pct_subscriptions_used  \n",
       "3                                 3050                   12.56  \n",
       "18                                2905                   12.40  \n",
       "25                                3055                   12.36  \n",
       "8                                 2860                   12.34  \n",
       "27                                2933                   12.34  \n",
       "31                                2802                   12.28  \n",
       "11                                2870                   12.22  \n",
       "2                                 3058                   12.20  \n",
       "6                                 2874                   12.20  \n",
       "36                                2852                   12.16  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# FIXED: baseline table + synthetic cleanup + realistic usage_date\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# A) Baseline feature adoption table (from df)\n",
    "# ----------------------------\n",
    "df_base = df.copy()\n",
    "\n",
    "# Clean feature names robustly (handles \"None\", None, \"\", whitespace)\n",
    "df_base[\"feature_name\"] = (\n",
    "    df_base[\"feature_name\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .replace({\"None\": pd.NA, \"\": pd.NA})\n",
    ")\n",
    "df_base = df_base.dropna(subset=[\"feature_name\"])\n",
    "\n",
    "# Safety: ensure numerics are numeric (prevents groupby sum/median issues)\n",
    "df_base[\"usage_count\"] = pd.to_numeric(df_base[\"usage_count\"], errors=\"coerce\").fillna(0)\n",
    "df_base[\"usage_duration_secs\"] = pd.to_numeric(df_base[\"usage_duration_secs\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Denominator\n",
    "total_subs = df_base[\"subscription_id\"].nunique()\n",
    "\n",
    "# Aggregate per (feature, subscription) so power users don't dominate\n",
    "feature_sub_agg = (\n",
    "    df_base\n",
    "    .groupby([\"feature_name\", \"subscription_id\"], as_index=False)\n",
    "    .agg(\n",
    "        total_usage_count=(\"usage_count\", \"sum\"),\n",
    "        total_usage_duration=(\"usage_duration_secs\", \"sum\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Feature-level baseline table\n",
    "feature_baseline = (\n",
    "    feature_sub_agg\n",
    "    .groupby(\"feature_name\", as_index=False)\n",
    "    .agg(\n",
    "        subscriptions_used=(\"subscription_id\", \"nunique\"),\n",
    "        median_usage_count_per_sub=(\"total_usage_count\", \"median\"),\n",
    "        median_usage_duration_secs_per_sub=(\"total_usage_duration\", \"median\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# % of subscriptions used\n",
    "feature_baseline[\"pct_subscriptions_used\"] = (\n",
    "    feature_baseline[\"subscriptions_used\"] / total_subs * 100\n",
    ").round(2)\n",
    "\n",
    "# Optional rounding/typing\n",
    "feature_baseline[\"median_usage_duration_secs_per_sub\"] = (\n",
    "    feature_baseline[\"median_usage_duration_secs_per_sub\"]\n",
    "    .fillna(0)\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "feature_baseline = feature_baseline.sort_values(\"pct_subscriptions_used\", ascending=False)\n",
    "\n",
    "# ----------------------------\n",
    "# B) Synthetic df cleanup + realistic usage_date generation\n",
    "# ----------------------------\n",
    "# Fix: str methods break if columns aren't strings; also \"sdv-\" replacement should be literal (regex=False)\n",
    "for col, prefix in [(\"usage_id\", \"syn_\"), (\"usage_pk\", \"sdv-\")]:\n",
    "    if col in synthetic_df.columns:\n",
    "        synthetic_df[col] = (\n",
    "            synthetic_df[col]\n",
    "            .astype(\"string\")\n",
    "            .str.replace(prefix, \"\", regex=False)\n",
    "        )\n",
    "\n",
    "# Create realistic usage_date based on one anchor per subscription + random offset\n",
    "start = pd.Timestamp(\"2023-01-02\")\n",
    "end   = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# one anchor date per subscription (normalized to date)\n",
    "subs = synthetic_df[[\"subscription_id\"]].drop_duplicates()\n",
    "\n",
    "anchors = subs.assign(\n",
    "    anchor=pd.to_datetime(\n",
    "        np.random.randint(\n",
    "            start.value // 10**9,\n",
    "            end.value   // 10**9,\n",
    "            size=len(subs)\n",
    "        ),\n",
    "        unit=\"s\",\n",
    "        utc=True,  # avoids timezone surprises\n",
    "    ).tz_convert(None).normalize()  # back to naive midnight\n",
    ")\n",
    "\n",
    "synthetic_df = synthetic_df.merge(anchors, on=\"subscription_id\", how=\"left\")\n",
    "\n",
    "# each event occurs 0–60 days after the anchor\n",
    "synthetic_df[\"usage_date\"] = (\n",
    "    synthetic_df[\"anchor\"] + pd.to_timedelta(np.random.randint(0, 61, size=len(synthetic_df)), unit=\"D\")\n",
    ")\n",
    "\n",
    "synthetic_df = synthetic_df.drop(columns=[\"anchor\"])\n",
    "\n",
    "# Preview\n",
    "feature_baseline.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebb399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df['usage_id'] = synthetic_df['usage_id'].str.replace('syn_', \"\", regex=False)\n",
    "synthetic_df['usage_pk'] = synthetic_df['usage_pk'].str.replace('sdv-',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86ca9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv(data_dir / 'feature_usage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0898ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05918119173084718"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_interacted = synthetic_df.loc[\n",
    "    synthetic_df['feature_name'] == 'feature_new_ai',\n",
    "    'subscription_id'\n",
    "].nunique() / synthetic_df['subscription_id'].nunique()\n",
    "\n",
    "percent_interacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b00f83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4934"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df['subscription_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d84c7",
   "metadata": {},
   "source": [
    "About 6% of subscriptions interacetd with the new feature 'feature_new_ai'. This is sufficient enough to be considered realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1fc524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir / 'ravenstack_subscriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f25d67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49d3672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.loc[\n",
    "    ~synthetic_df['subscription_id'].isin(df['subscription_id']),\n",
    "    'subscription_id'\n",
    "].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afafe7",
   "metadata": {},
   "source": [
    "I see that each subscription_id that is in synthetic appears at least once in the subscriptions.csv . Therefore, we can get link any subscription_id from feature_usage.csv to subcriptions.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
