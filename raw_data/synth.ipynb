{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced5ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca94039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch-directml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013224f",
   "metadata": {},
   "source": [
    "### This file is used to synthesize data that mimics the existing patterns of a dataset. For project purposes sake, we can synthesize data to more realistically represent real-world data. \n",
    "\n",
    "This currently only applies to the feature_usage.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import uuid\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.sampling import Condition\n",
    "import torch\n",
    "import torch_directml\n",
    "\n",
    "\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "base_dir = Path(\"C:\\\\Users\\\\henry\\\\OneDrive\\\\Personal Career\\\\Personal Projects\\\\GitHub\\\\Revenue-Sustainability-Analysis\")\n",
    "data_dir = Path(base_dir / \"raw_data/Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c11491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bce3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM '{data_dir}/ravenstack_feature_usage.csv'\n",
    "                 \"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b05c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Using device: privateuseone:0\n",
      "Tensor on AMD GPU: tensor([1., 1., 1.], device='privateuseone:0')\n"
     ]
    }
   ],
   "source": [
    "# Check if DirectML is available\n",
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "    print(f\"Success! Using device: {device}\")\n",
    "    \n",
    "    # Create a tensor on your AMD GPU\n",
    "    x = torch.ones(3).to(device)\n",
    "    print(\"Tensor on AMD GPU:\", x)\n",
    "else:\n",
    "    print(\"DirectML not found. Check your installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01238bd8",
   "metadata": {},
   "source": [
    "### Introduce synthetic data to feature_usage since we do not have enough rows to replicate realistic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2fc06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sdv\\metadata\\single_table.py:835: UserWarning: There is an existing primary key 'usage_pk'. This key will be removed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sdv\\single_table\\base.py:134: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Sampling conditions: 100%|██████████| 198/198 [00:00<00:00, 2211.73it/s]\n"
     ]
    }
   ],
   "source": [
    "RELEASE_TS = pd.Timestamp(\"2025-10-01\")\n",
    "INTRODUCED_FEATURE = \"feature_new_ai\"\n",
    "TOTAL_ROWS = 50000\n",
    "POST_RELEASE_LIFT = 3.0\n",
    "TABLE = \"usage_events\"\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# types\n",
    "df[\"usage_date\"] = pd.to_datetime(df[\"usage_date\"], errors=\"coerce\")\n",
    "df[\"feature_name\"] = df[\"feature_name\"].replace(\"None\", pd.NA)\n",
    "df = df.dropna(subset=[\"feature_name\"])\n",
    "\n",
    "df[\"subscription_id\"] = df[\"subscription_id\"].astype(str)\n",
    "df[\"feature_name\"] = df[\"feature_name\"].astype(str)\n",
    "df[\"is_beta_feature\"] = df[\"is_beta_feature\"].astype(bool)\n",
    "\n",
    "for c in [\"usage_count\", \"usage_duration_secs\", \"error_count\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# release flag\n",
    "df[\"post_release\"] = (df[\"usage_date\"] >= RELEASE_TS)\n",
    "\n",
    "# ensure introduced feature exists\n",
    "if INTRODUCED_FEATURE not in set(df[\"feature_name\"].unique()):\n",
    "    seed_n = max(10, int(0.002 * len(df)))\n",
    "    seed = df.sample(seed_n, random_state=42).copy()\n",
    "    seed[\"feature_name\"] = INTRODUCED_FEATURE\n",
    "    seed[\"post_release\"] = True\n",
    "    seed[\"is_beta_feature\"] = False\n",
    "    df = pd.concat([df, seed], ignore_index=True)\n",
    "\n",
    "# CRITICAL: create guaranteed-unique PK AFTER all concatenations\n",
    "df[\"usage_pk\"] = [uuid.uuid4().hex for _ in range(len(df))]\n",
    "\n",
    "# metadata\n",
    "metadata = Metadata.detect_from_dataframe(data=df, table_name=TABLE)\n",
    "metadata.update_column(table_name=TABLE, column_name=\"usage_pk\", sdtype=\"id\")\n",
    "metadata.set_primary_key(table_name=TABLE, column_name=\"usage_pk\")\n",
    "metadata.update_column(table_name=TABLE, column_name=\"usage_date\", sdtype=\"datetime\")\n",
    "\n",
    "# fit\n",
    "synth = GaussianCopulaSynthesizer(metadata, enforce_min_max_values=True, enforce_rounding=True)\n",
    "synth.fit(df)\n",
    "\n",
    "# sample baseline\n",
    "syn_base = synth.sample(num_rows=TOTAL_ROWS)\n",
    "\n",
    "# oversample introduced feature post-release\n",
    "post_df = df[df[\"post_release\"] == True]\n",
    "post_share = len(post_df) / len(df) if len(df) else 0.5\n",
    "post_n = int(TOTAL_ROWS * post_share)\n",
    "\n",
    "base_feat_share_post = (post_df[\"feature_name\"] == INTRODUCED_FEATURE).mean() if len(post_df) else 0.0\n",
    "base_feat_n_post = int(post_n * base_feat_share_post)\n",
    "target_feat_n_post = int(base_feat_n_post * POST_RELEASE_LIFT)\n",
    "add_n = max(0, target_feat_n_post - base_feat_n_post)\n",
    "\n",
    "if add_n > 0:\n",
    "    cond = Condition({\"post_release\": True, \"feature_name\": INTRODUCED_FEATURE}, num_rows=add_n)\n",
    "    syn_lift = synth.sample_from_conditions([cond])\n",
    "    synthetic_df = pd.concat([syn_base, syn_lift], ignore_index=True)\n",
    "else:\n",
    "    synthetic_df = syn_base.copy()\n",
    "\n",
    "# final unique id for synthetic dataset\n",
    "synthetic_df[\"usage_id\"] = [f\"syn_usage_{i}\" for i in range(len(synthetic_df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2b6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# A) Baseline feature adoption table (from df)\n",
    "# ----------------------------\n",
    "df_base = df.copy()\n",
    "\n",
    "# Clean feature names robustly (handles \"None\", None, \"\", whitespace)\n",
    "df_base[\"feature_name\"] = (\n",
    "    df_base[\"feature_name\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .replace({\"None\": pd.NA, \"\": pd.NA})\n",
    ")\n",
    "df_base = df_base.dropna(subset=[\"feature_name\"])\n",
    "\n",
    "# Safety: ensure numerics are numeric (prevents groupby sum/median issues)\n",
    "df_base[\"usage_count\"] = pd.to_numeric(df_base[\"usage_count\"], errors=\"coerce\").fillna(0)\n",
    "df_base[\"usage_duration_secs\"] = pd.to_numeric(df_base[\"usage_duration_secs\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Denominator\n",
    "total_subs = df_base[\"subscription_id\"].nunique()\n",
    "\n",
    "# Aggregate per (feature, subscription) so power users don't dominate\n",
    "feature_sub_agg = (\n",
    "    df_base\n",
    "    .groupby([\"feature_name\", \"subscription_id\"], as_index=False)\n",
    "    .agg(\n",
    "        total_usage_count=(\"usage_count\", \"sum\"),\n",
    "        total_usage_duration=(\"usage_duration_secs\", \"sum\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Feature-level baseline table\n",
    "feature_baseline = (\n",
    "    feature_sub_agg\n",
    "    .groupby(\"feature_name\", as_index=False)\n",
    "    .agg(\n",
    "        subscriptions_used=(\"subscription_id\", \"nunique\"),\n",
    "        median_usage_count_per_sub=(\"total_usage_count\", \"median\"),\n",
    "        median_usage_duration_secs_per_sub=(\"total_usage_duration\", \"median\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# % of subscriptions used\n",
    "feature_baseline[\"pct_subscriptions_used\"] = (\n",
    "    feature_baseline[\"subscriptions_used\"] / total_subs * 100\n",
    ").round(2)\n",
    "\n",
    "# Optional rounding/typing\n",
    "feature_baseline[\"median_usage_duration_secs_per_sub\"] = (\n",
    "    feature_baseline[\"median_usage_duration_secs_per_sub\"]\n",
    "    .fillna(0)\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "feature_baseline = feature_baseline.sort_values(\"pct_subscriptions_used\", ascending=False)\n",
    "\n",
    "# ----------------------------\n",
    "# B) Synthetic df cleanup + realistic usage_date generation\n",
    "# ----------------------------\n",
    "# Fix: str methods break if columns aren't strings; also \"sdv-\" replacement should be literal (regex=False)\n",
    "for col, prefix in [(\"usage_id\", \"syn_\"), (\"usage_pk\", \"sdv-\")]:\n",
    "    if col in synthetic_df.columns:\n",
    "        synthetic_df[col] = (\n",
    "            synthetic_df[col]\n",
    "            .astype(\"string\")\n",
    "            .str.replace(prefix, \"\", regex=False)\n",
    "        )\n",
    "\n",
    "# Create realistic usage_date based on one anchor per subscription + random offset\n",
    "start = pd.Timestamp(\"2023-06-12\")\n",
    "end   = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# one anchor date per subscription (normalized to date)\n",
    "subs = synthetic_df[[\"subscription_id\"]].drop_duplicates()\n",
    "\n",
    "anchors = subs.assign(\n",
    "    anchor=pd.to_datetime(\n",
    "        np.random.randint(\n",
    "            start.value // 10**9,\n",
    "            end.value   // 10**9,\n",
    "            size=len(subs)\n",
    "        ),\n",
    "        unit=\"s\",\n",
    "        utc=True,  # avoids timezone surprises\n",
    "    ).tz_convert(None).normalize()  # back to naive midnight\n",
    ")\n",
    "\n",
    "synthetic_df = synthetic_df.merge(anchors, on=\"subscription_id\", how=\"left\")\n",
    "\n",
    "# each event occurs 0–60 days after the anchor\n",
    "synthetic_df[\"usage_date\"] = (\n",
    "    synthetic_df[\"anchor\"] + pd.to_timedelta(np.random.randint(0, 61, size=len(synthetic_df)), unit=\"D\")\n",
    ")\n",
    "\n",
    "synthetic_df = synthetic_df.drop(columns=[\"anchor\"])\n",
    "\n",
    "synthetic_df['usage_id'] = synthetic_df['usage_id'].str.replace('syn_', \"\", regex=False)\n",
    "synthetic_df['usage_pk'] = synthetic_df['usage_pk'].str.replace('sdv-',\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f24ee8",
   "metadata": {},
   "source": [
    "### Ensure that we have enough usage dates for our new feature, just so that we have realistic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7233a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = synthetic_df\n",
    "\n",
    "users_once = (\n",
    "    df[df[\"feature_name\"] == \"feature_newai\"]\n",
    "    [\"subscription_id\"]\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "synthetic_rows = []\n",
    "\n",
    "for user in users_once:\n",
    "    base_date = df.loc[\n",
    "        df[\"subscription_id\"] == user, \"usage_day\"\n",
    "    ].dropna().min()\n",
    "\n",
    "    if pd.isna(base_date):\n",
    "        continue\n",
    "\n",
    "    roll = np.random.rand()\n",
    "\n",
    "    if roll < 0.6:\n",
    "        days = 1\n",
    "    elif roll < 0.85:\n",
    "        days = 2\n",
    "    else:\n",
    "        days = np.random.randint(3, 6)\n",
    "\n",
    "    for d in range(1, days):\n",
    "        synthetic_rows.append({\n",
    "            \"subscription_id\": user,\n",
    "            \"feature_name\": \"feature_newai\",\n",
    "            \"usage_day\": base_date + timedelta(days=d),\n",
    "            \"synthetic\": True\n",
    "        })\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_rows)\n",
    "df_augmented = pd.concat([df, synthetic_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74f9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: feature-specific rows are sparse. Training SDV on broader data: 50198 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sdv\\single_table\\base.py:168: FutureWarning: The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "  warnings.warn(DEPRECATION_MSG, FutureWarning)\n",
      "c:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sdv\\single_table\\base.py:134: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# SETTINGS YOU CAN TWEAK\n",
    "# -----------------------------\n",
    "FEATURE = \"feature_newai\"\n",
    "\n",
    "ADD_NEW_USERS = 200        # reach: how many brand-new subscription_ids to add with 1-day usage\n",
    "PROMOTE_USERS = 150        # depth: how many users (existing + new) to push into 2/3/4 days\n",
    "\n",
    "# adoption distribution for promoted users (must sum to 1.0)\n",
    "# \"Most 1 day\" applies naturally; promotion assigns targets for a subset\n",
    "P_2DAY = 0.70              # most promoted -> 2 days\n",
    "P_3DAY = 0.25              # fewer -> 3 days\n",
    "P_4DAY = 0.05              # only a few -> 4 days\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) CLEAN DATES\n",
    "# -----------------------------\n",
    "feats = df_augmented.copy()  # (overwrites variable feats; keeps your original object safe if needed)\n",
    "feats[\"synthetic\"] = feats.get(\"synthetic\", False)\n",
    "\n",
    "feats[\"usage_date\"] = feats[\"usage_date\"].astype(str).str.strip()\n",
    "feats[\"usage_date\"] = pd.to_datetime(feats[\"usage_date\"], errors=\"coerce\")\n",
    "feats[\"_usage_day\"] = feats[\"usage_date\"].dt.normalize()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) TRAIN SDV ON EXISTING feature_newai ROWS (fallback to all rows if too few)\n",
    "# -----------------------------\n",
    "train = feats[(feats[\"feature_name\"] == FEATURE) & feats[\"subscription_id\"].notna() & feats[\"_usage_day\"].notna()].copy()\n",
    "\n",
    "# If too sparse, SDV will learn garbage. Fallback to all rows (still SDV, just broader behavior).\n",
    "if len(train) < 50:\n",
    "    train = feats[feats[\"subscription_id\"].notna() & feats[\"_usage_day\"].notna()].copy()\n",
    "    print(f\"Warning: feature-specific rows are sparse. Training SDV on broader data: {len(train)} rows.\")\n",
    "\n",
    "exclude_cols = {\"subscription_id\", \"usage_id\", \"usage_pk\", \"synthetic\"}\n",
    "model_cols = [c for c in train.columns if c not in exclude_cols and c != \"_usage_day\"]\n",
    "\n",
    "model_df = train[model_cols].copy()\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(model_df)\n",
    "\n",
    "synth = GaussianCopulaSynthesizer(metadata)\n",
    "synth.fit(model_df)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) DATE WINDOW FOR REALISM\n",
    "# -----------------------------\n",
    "global_min = feats[\"_usage_day\"].min()\n",
    "global_max = feats[\"_usage_day\"].max()\n",
    "if pd.isna(global_min) or pd.isna(global_max):\n",
    "    raise ValueError(\"No valid date window after parsing usage_date.\")\n",
    "\n",
    "date_range = pd.date_range(global_min, global_max, freq=\"D\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) HELPERS\n",
    "# -----------------------------\n",
    "def make_sub_ids(n, existing):\n",
    "    existing = set(existing)\n",
    "    new_ids = []\n",
    "    while len(new_ids) < n:\n",
    "        cid = \"S-\" + uuid.uuid4().hex[:6]\n",
    "        if cid not in existing:\n",
    "            existing.add(cid)\n",
    "            new_ids.append(cid)\n",
    "    return new_ids\n",
    "\n",
    "def make_usage_id():\n",
    "    return \"U-syn\" + uuid.uuid4().hex[:8]\n",
    "\n",
    "def make_usage_pk():\n",
    "    return \"id_syn_\" + uuid.uuid4().hex[:10]\n",
    "\n",
    "def pick_target_days(n):\n",
    "    # promoted users get 2/3/4 day targets only, per distribution\n",
    "    choices = np.array([2, 3, 4])\n",
    "    probs = np.array([P_2DAY, P_3DAY, P_4DAY])\n",
    "    probs = probs / probs.sum()\n",
    "    return rng.choice(choices, size=n, replace=True, p=probs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) REACH: ADD NEW USERS (each gets exactly 1 day of feature usage)\n",
    "# -----------------------------\n",
    "existing_all_users = feats[\"subscription_id\"].dropna().unique()\n",
    "new_users = make_sub_ids(ADD_NEW_USERS, existing_all_users)\n",
    "\n",
    "new_user_rows = synth.sample(ADD_NEW_USERS)\n",
    "\n",
    "# enforce feature\n",
    "new_user_rows[\"feature_name\"] = FEATURE\n",
    "\n",
    "# assign each new user exactly one usage day\n",
    "new_days = rng.choice(date_range, size=ADD_NEW_USERS, replace=True)\n",
    "new_user_rows[\"usage_date\"] = pd.to_datetime(new_days)\n",
    "\n",
    "# assign identifiers\n",
    "new_user_rows[\"subscription_id\"] = new_users\n",
    "new_user_rows[\"usage_id\"] = [make_usage_id() for _ in range(ADD_NEW_USERS)]\n",
    "new_user_rows[\"usage_pk\"] = [make_usage_pk() for _ in range(ADD_NEW_USERS)]\n",
    "new_user_rows[\"synthetic\"] = True\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) DEPTH: PROMOTE USERS TO 2/3/4 DISTINCT DAYS (NEVER > 4)\n",
    "# -----------------------------\n",
    "# Build current used-day sets for feature users\n",
    "feat_only = feats[(feats[\"feature_name\"] == FEATURE) & feats[\"subscription_id\"].notna() & feats[\"_usage_day\"].notna()].copy()\n",
    "used_days = feat_only.groupby(\"subscription_id\")[\"_usage_day\"].apply(lambda s: set(s.tolist())).to_dict()\n",
    "\n",
    "# include new users as having 1 day already\n",
    "for uid, day in zip(new_users, pd.to_datetime(new_days).normalize()):\n",
    "    used_days[uid] = set([day])\n",
    "\n",
    "# promotion pool: users with <4 distinct days (existing + new)\n",
    "promotion_pool = [u for u, days in used_days.items() if len(days) < 4]\n",
    "if len(promotion_pool) == 0:\n",
    "    print(\"No users eligible for promotion (<4 days). Skipping depth synthesis.\")\n",
    "    promo_rows = pd.DataFrame()\n",
    "else:\n",
    "    rng.shuffle(promotion_pool)\n",
    "    promote_n = min(PROMOTE_USERS, len(promotion_pool))\n",
    "    promote_users = promotion_pool[:promote_n]\n",
    "\n",
    "    targets = pick_target_days(promote_n)\n",
    "\n",
    "    promo_records = []\n",
    "    for uid, target in zip(promote_users, targets):\n",
    "        current_days = used_days.get(uid, set())\n",
    "        # cap target at 4 and never below current\n",
    "        target = int(min(4, max(target, len(current_days))))\n",
    "        needed = target - len(current_days)\n",
    "        if needed <= 0:\n",
    "            continue\n",
    "\n",
    "        # sample event templates via SDV\n",
    "        templates = synth.sample(needed)\n",
    "        templates[\"feature_name\"] = FEATURE\n",
    "        templates[\"subscription_id\"] = uid\n",
    "        templates[\"synthetic\"] = True\n",
    "        templates[\"usage_id\"] = [make_usage_id() for _ in range(needed)]\n",
    "        templates[\"usage_pk\"] = [make_usage_pk() for _ in range(needed)]\n",
    "\n",
    "        # choose NEW distinct days for this user (after their earliest day)\n",
    "        base_day = min(current_days) if current_days else global_min\n",
    "        new_distinct = set()\n",
    "\n",
    "        attempts = 0\n",
    "        while len(new_distinct) < needed and attempts < 5000:\n",
    "            # keep repeats realistic: within 1–30 days after first use\n",
    "            offset = int(rng.integers(1, 31))\n",
    "            cand = (base_day + pd.Timedelta(days=offset)).normalize()\n",
    "            if cand > global_max:\n",
    "                cand = global_max.normalize()\n",
    "\n",
    "            if cand not in current_days and cand not in new_distinct:\n",
    "                new_distinct.add(cand)\n",
    "            attempts += 1\n",
    "\n",
    "        new_distinct = sorted(new_distinct)\n",
    "        templates[\"usage_date\"] = pd.to_datetime(new_distinct)\n",
    "\n",
    "        # update tracking\n",
    "        used_days[uid] = current_days.union(new_distinct)\n",
    "\n",
    "        promo_records.append(templates)\n",
    "\n",
    "    promo_rows = pd.concat(promo_records, ignore_index=True) if promo_records else pd.DataFrame()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) APPEND SYNTHETIC ROWS INTO feats (overwrite feats variable)\n",
    "# -----------------------------\n",
    "to_add = pd.concat([new_user_rows, promo_rows], ignore_index=True)\n",
    "\n",
    "# align columns\n",
    "for c in feats.columns:\n",
    "    if c not in to_add.columns:\n",
    "        to_add[c] = np.nan\n",
    "to_add = to_add[feats.columns]\n",
    "\n",
    "feats = pd.concat([feats.drop(columns=[\"_usage_day\"]), to_add], ignore_index=True)\n",
    "\n",
    "# recompute day helper (optional, but nice)\n",
    "feats[\"usage_date\"] = pd.to_datetime(feats[\"usage_date\"], errors=\"coerce\")\n",
    "feats[\"usage_day\"] = feats[\"usage_date\"].dt.normalize()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8) VALIDATE DISTRIBUTION\n",
    "# -----------------------------\n",
    "user_days = (\n",
    "    feats[feats[\"feature_name\"] == FEATURE]\n",
    "    .dropna(subset=[\"subscription_id\", \"usage_day\"])\n",
    "    .groupby(\"subscription_id\")[\"usage_day\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"distinct_days\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86ca9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.to_csv(data_dir / 'feature_usage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c12f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usage_id</th>\n",
       "      <th>subscription_id</th>\n",
       "      <th>usage_date</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>usage_count</th>\n",
       "      <th>usage_duration_secs</th>\n",
       "      <th>error_count</th>\n",
       "      <th>is_beta_feature</th>\n",
       "      <th>post_release</th>\n",
       "      <th>usage_pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usage_0</td>\n",
       "      <td>S-38131a</td>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>feature_25</td>\n",
       "      <td>14</td>\n",
       "      <td>3782</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>id-gKDvFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usage_1</td>\n",
       "      <td>S-712e69</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>feature_30</td>\n",
       "      <td>8</td>\n",
       "      <td>2553</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>id-GPcfRg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usage_2</td>\n",
       "      <td>S-9c09cb</td>\n",
       "      <td>2024-08-18</td>\n",
       "      <td>feature_13</td>\n",
       "      <td>13</td>\n",
       "      <td>1860</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>id-DFPSav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>usage_3</td>\n",
       "      <td>S-2dbf26</td>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>feature_33</td>\n",
       "      <td>9</td>\n",
       "      <td>834</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>id-QgdjPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usage_4</td>\n",
       "      <td>S-d114e2</td>\n",
       "      <td>2024-06-12</td>\n",
       "      <td>feature_4</td>\n",
       "      <td>9</td>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>id-xbbuep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50193</th>\n",
       "      <td>usage_50193</td>\n",
       "      <td>S-104941</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>feature_new_ai</td>\n",
       "      <td>8</td>\n",
       "      <td>2211</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>id-BqttoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50194</th>\n",
       "      <td>usage_50194</td>\n",
       "      <td>S-5cb8a6</td>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>feature_new_ai</td>\n",
       "      <td>11</td>\n",
       "      <td>5980</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>id-nYuaKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50195</th>\n",
       "      <td>usage_50195</td>\n",
       "      <td>S-b7ef4c</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>feature_new_ai</td>\n",
       "      <td>5</td>\n",
       "      <td>702</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>id-VBoAzR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50196</th>\n",
       "      <td>usage_50196</td>\n",
       "      <td>S-d11408</td>\n",
       "      <td>2024-11-17</td>\n",
       "      <td>feature_new_ai</td>\n",
       "      <td>6</td>\n",
       "      <td>3684</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>id-IVdsls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50197</th>\n",
       "      <td>usage_50197</td>\n",
       "      <td>S-396a83</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>feature_new_ai</td>\n",
       "      <td>10</td>\n",
       "      <td>2685</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>id-vNQHig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50198 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          usage_id subscription_id usage_date    feature_name  usage_count  \\\n",
       "0          usage_0        S-38131a 2024-12-11      feature_25           14   \n",
       "1          usage_1        S-712e69 2023-10-09      feature_30            8   \n",
       "2          usage_2        S-9c09cb 2024-08-18      feature_13           13   \n",
       "3          usage_3        S-2dbf26 2024-09-19      feature_33            9   \n",
       "4          usage_4        S-d114e2 2024-06-12       feature_4            9   \n",
       "...            ...             ...        ...             ...          ...   \n",
       "50193  usage_50193        S-104941 2023-12-20  feature_new_ai            8   \n",
       "50194  usage_50194        S-5cb8a6 2024-01-23  feature_new_ai           11   \n",
       "50195  usage_50195        S-b7ef4c 2023-12-07  feature_new_ai            5   \n",
       "50196  usage_50196        S-d11408 2024-11-17  feature_new_ai            6   \n",
       "50197  usage_50197        S-396a83 2024-12-23  feature_new_ai           10   \n",
       "\n",
       "       usage_duration_secs  error_count  is_beta_feature  post_release  \\\n",
       "0                     3782            2            False         False   \n",
       "1                     2553            0             True         False   \n",
       "2                     1860            0            False         False   \n",
       "3                      834            2            False         False   \n",
       "4                     1300            1            False         False   \n",
       "...                    ...          ...              ...           ...   \n",
       "50193                 2211            2            False          True   \n",
       "50194                 5980            0            False          True   \n",
       "50195                  702            1             True          True   \n",
       "50196                 3684            0             True          True   \n",
       "50197                 2685            0            False          True   \n",
       "\n",
       "        usage_pk  \n",
       "0      id-gKDvFC  \n",
       "1      id-GPcfRg  \n",
       "2      id-DFPSav  \n",
       "3      id-QgdjPG  \n",
       "4      id-xbbuep  \n",
       "...          ...  \n",
       "50193  id-BqttoS  \n",
       "50194  id-nYuaKD  \n",
       "50195  id-VBoAzR  \n",
       "50196  id-IVdsls  \n",
       "50197  id-vNQHig  \n",
       "\n",
       "[50198 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0898ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05918119173084718"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_interacted = df_augmented.loc[\n",
    "    df_augmented['feature_name'] == 'feature_new_ai',\n",
    "    'subscription_id'\n",
    "].nunique() / df_augmented['subscription_id'].nunique()\n",
    "\n",
    "percent_interacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b00f83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4934"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented['subscription_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d84c7",
   "metadata": {},
   "source": [
    "About 6% of subscriptions interacetd with the new feature 'feature_new_ai'. This is sufficient enough to be considered realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1fc524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir / 'ravenstack_subscriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25d67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d3672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented.loc[\n",
    "    ~df_augmented['subscription_id'].isin(df['subscription_id']),\n",
    "    'subscription_id'\n",
    "].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afafe7",
   "metadata": {},
   "source": [
    "I see that each subscription_id that is in synthetic appears at least once in the subscriptions.csv . Therefore, we can get link any subscription_id from feature_usage.csv to subcriptions.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
